{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    # set headers and response variables\n",
    "    headers = {'User-Agent': 'Codeup Data Science'} \n",
    "    response = get(url, headers=headers)\n",
    "    # use BeartifulSoup to make object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def github_geology_urls():\n",
    "    '''\n",
    "    This function scrapes all of the urls from\n",
    "    the github search page and returns a list of the most recently updated Geology urls.\n",
    "    '''\n",
    "    # get the first 500 pages to allow for those that don't have readme or language\n",
    "    pages = range(1, 500)\n",
    "    urls = []\n",
    "    \n",
    "    for p in pages:\n",
    "        \n",
    "        # format string of the base url for the main github search page we are using to update with page number\n",
    "        url = f'https://github.com/search?%7Bp%7Do=desc&p={p}&q=geology&s=updated&type=Repositories'   \n",
    "\n",
    "        # Make request and soup object using helper\n",
    "        soup = make_soup(url)\n",
    "\n",
    "        # Create a list of the anchor elements that hold the urls on this search page\n",
    "        page_urls_list = soup.find_all('a', class_='v-align-middle')\n",
    "        # for each url in the find all list get just the 'href' link\n",
    "        page_urls = {link.get('href') for link in page_urls_list}\n",
    "        # make a list of these urls\n",
    "        page_urls = list(page_urls)\n",
    "        # append the list from the page to the full list to return\n",
    "        urls.append(page_urls)\n",
    "        time.sleep(5)\n",
    "    # flatten the urls list\n",
    "    urls = [y for x in urls for y in x]\n",
    "    return urls\n",
    "\n",
    "def get_geo_results(cached=False):\n",
    "    '''\n",
    "    This function with default cached == False does a fresh scrape of github pages returned from\n",
    "    search of 'environmental' and writes the returned df to a json file.\n",
    "    cached == True returns a df read in from a json file.\n",
    "    '''\n",
    "    # option to read in a json file instead of scrape for df\n",
    "    if cached == True:\n",
    "        df = pd.read_json('readgeo.json')\n",
    "        \n",
    "    # cached == False completes a fresh scrape for df    \n",
    "    else:\n",
    "        # get url list\n",
    "        url_list = github_geology_urls()\n",
    "\n",
    "        # Set base_url that will be used in get request\n",
    "        base_url = 'https://github.com'\n",
    "        \n",
    "        # List of full url needed to get readme info\n",
    "        readme_url_list = []\n",
    "        for url in url_list:\n",
    "            full_url = base_url + url\n",
    "            readme_url_list.append(full_url)\n",
    "        \n",
    "        # Create an empty list, readmes, to hold our dictionaries\n",
    "        readmes = []\n",
    "\n",
    "        for readme_url in readme_url_list:\n",
    "            # Make request and soup object using helper\n",
    "            soup = make_soup(readme_url)\n",
    "\n",
    "            if soup.find('article', class_=\"markdown-body entry-content container-lg\") != None:            \n",
    "                # Save the text in each readme to variable text\n",
    "                content = soup.find('article', class_=\"markdown-body entry-content container-lg\").text\n",
    "            \n",
    "            if soup.find('span', class_=\"text-gray-dark text-bold mr-1\") != None:\n",
    "            # Save the first language in each readme to variable text\n",
    "                # NOTE: this is the majority language, not all of the languages used\n",
    "                language = soup.find('span', class_=\"text-gray-dark text-bold mr-1\").text\n",
    "\n",
    "                # anything else useful on the page?\n",
    "\n",
    "                # Create a dictionary holding the title and content for each blog\n",
    "                readme = {'language': language, 'content': content}\n",
    "\n",
    "                # Add each dictionary to the articles list of dictionaries\n",
    "                readmes.append(readme)\n",
    "            \n",
    "        # convert our list of dictionaries to a df\n",
    "        df = pd.DataFrame(readmes)\n",
    "\n",
    "        # Write df to a json file for faster access\n",
    "        df.to_json('readgeo.json')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "      <td>Map Merger tool - tested using ArcMap 10.7\\nWr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>wellio.js\\nJavaScript for converting well-log ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python</td>\n",
       "      <td>geomodel-2-3dweb\\n\\nGenerates 3D web versions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>GeoFeature\\nGeological features of the Quanfoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>U.S. Geological Survey Best Practices\\nThis re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           language                                            content\n",
       "0            Python  Map Merger tool - tested using ArcMap 10.7\\nWr...\n",
       "1  Jupyter Notebook  wellio.js\\nJavaScript for converting well-log ...\n",
       "2            Python  geomodel-2-3dweb\\n\\nGenerates 3D web versions ...\n",
       "3        JavaScript  GeoFeature\\nGeological features of the Quanfoc...\n",
       "4        JavaScript  U.S. Geological Survey Best Practices\\nThis re..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_geo_results(cached=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript           97\n",
       "Python               97\n",
       "Jupyter Notebook     82\n",
       "HTML                 81\n",
       "Java                 46\n",
       "R                    13\n",
       "C++                  13\n",
       "C#                   10\n",
       "MATLAB                8\n",
       "PHP                   7\n",
       "TypeScript            6\n",
       "CSS                   5\n",
       "TeX                   5\n",
       "Fortran               4\n",
       "Ruby                  3\n",
       "C                     3\n",
       "Shell                 2\n",
       "Go                    2\n",
       "CoffeeScript          2\n",
       "Kotlin                2\n",
       "VBA                   2\n",
       "QML                   1\n",
       "Elixir                1\n",
       "Perl                  1\n",
       "Rich Text Format      1\n",
       "OCaml                 1\n",
       "F#                    1\n",
       "SCSS                  1\n",
       "Pascal                1\n",
       "PostScript            1\n",
       "Visual Basic .NET     1\n",
       "Batchfile             1\n",
       "Swift                 1\n",
       "Rust                  1\n",
       "SAS                   1\n",
       "Objective-C           1\n",
       "Vue                   1\n",
       "Lua                   1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
