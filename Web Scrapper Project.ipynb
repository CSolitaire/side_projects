{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acquire.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# scroll down for exercise functions\n",
    "\n",
    "###### project functions #########\n",
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    # set headers and response variables\n",
    "    headers = {'User-Agent': 'Codeup Data Science'} \n",
    "    response = get(url, headers=headers)\n",
    "    # use BeartifulSoup to make object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def github_HP_urls():\n",
    "    '''\n",
    "    This function scrapes all of the Harry Potter urls from\n",
    "    the github search page and returns a list of urls.\n",
    "    '''\n",
    "    # get the first 100 pages to allow for those that don't have readme or language\n",
    "    pages = range(1, 101)\n",
    "    urls = []\n",
    "    \n",
    "    for p in pages:\n",
    "        \n",
    "        # format string of the base url for the main github search page we are using to update with page number\n",
    "        url = f'https://github.com/search?p={p}&q=harry+potter&type=Repositories'\n",
    "\n",
    "        # Make request and soup object using helper\n",
    "        soup = make_soup(url)\n",
    "\n",
    "        # Create a list of the anchor elements that hold the urls on this search page\n",
    "        page_urls_list = soup.find_all('a', class_='v-align-middle')\n",
    "        # for each url in the find all list get just the 'href' link\n",
    "        page_urls = {link.get('href') for link in page_urls_list}\n",
    "        # make a list of these urls\n",
    "        page_urls = list(page_urls)\n",
    "        # append the list from the page to the full list to return\n",
    "        urls.append(page_urls)\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # flatten the urls list\n",
    "    urls = [y for x in urls for y in x]\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "def github_urls_single_page():\n",
    "    '''\n",
    "    This function scrapes all of the evironmental urls from\n",
    "    the github first search page and returns a list of urls.\n",
    "    '''\n",
    "    # The base url for the main github search page we are using\n",
    "    url = 'https://github.com/search?o=desc&p=1&q=environmental&s=&type=Repositories'\n",
    "    \n",
    "    # Make request and soup object using helper\n",
    "    soup = make_soup(url)\n",
    "    \n",
    "    # Create a list of the anchor elements that hold the urls.\n",
    "    urls_list = soup.find_all('a', class_='v-align-middle')\n",
    "    # for each url in the find all list get just the 'href' link\n",
    "    urls = {link.get('href') for link in urls_list}\n",
    "    # make a list of these urls\n",
    "    urls = list(urls)\n",
    "    return urls\n",
    "# this gets 1st 10 urls, will need to get next 10 pages\n",
    "\n",
    "def get_github_HPresults(cached=False):\n",
    "    '''\n",
    "    This function with default cached == False does a fresh scrape of github pages returned from\n",
    "    search of 'environmental' and writes the returned df to a json file.\n",
    "    cached == True returns a df read in from a json file.\n",
    "    '''\n",
    "    # option to read in a json file instead of scrape for df\n",
    "    if cached == True:\n",
    "        df = pd.read_json('readmes.json')\n",
    "        \n",
    "    # cached == False completes a fresh scrape for df    \n",
    "    else:\n",
    "        # get url list\n",
    "        url_list = github_HP_urls()\n",
    "\n",
    "        # Set base_url that will be used in get request\n",
    "        base_url = 'https://github.com'\n",
    "        \n",
    "        # List of full url needed to get readme info\n",
    "        readme_url_list = []\n",
    "        for url in url_list:\n",
    "            full_url = base_url + url\n",
    "            readme_url_list.append(full_url)\n",
    "        \n",
    "        # Create an empty list, readmes, to hold our dictionaries\n",
    "        readmes = []\n",
    "\n",
    "        for readme_url in readme_url_list:\n",
    "            # Make request and soup object using helper\n",
    "            soup = make_soup(readme_url)\n",
    "\n",
    "            if soup.find('article', class_=\"markdown-body entry-content container-lg\") != None:            \n",
    "                # Save the text in each readme to variable text\n",
    "                content = soup.find('article', class_=\"markdown-body entry-content container-lg\").text\n",
    "            \n",
    "            if soup.find('span', class_=\"text-gray-dark text-bold mr-1\") != None:\n",
    "            # Save the first language in each readme to variable text\n",
    "                # NOTE: this is the majority language, not all of the languages used\n",
    "                language = soup.find('span', class_=\"text-gray-dark text-bold mr-1\").text\n",
    "\n",
    "                # anything else useful on the page?\n",
    "\n",
    "                # Create a dictionary holding the title and content for each blog\n",
    "                readme = {'language': language, 'content': content}\n",
    "\n",
    "                # Add each dictionary to the articles list of dictionaries\n",
    "                readmes.append(readme)\n",
    "            \n",
    "        # convert our list of dictionaries to a df\n",
    "        df = pd.DataFrame(readmes)\n",
    "\n",
    "        # Write df to a json file for faster access\n",
    "        df.to_json('readmes.json')\n",
    "\n",
    "    return df\n",
    "    # 339 observations with 50 pgs\n",
    "    # ... observations with 100 pgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'acquire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2be7827d5e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'acquire'"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    '''\n",
    "    Initial basic cleaning/normalization of text string\n",
    "    '''\n",
    "    # change to all lowercase\n",
    "    low_case = text.lower()\n",
    "    # remove special characters, encode to ascii and recode to utf-8\n",
    "    recode = unicodedata.normalize('NFKD', low_case).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # Replace anything that is not a letter, number, whitespace or a single quote\n",
    "    cleaned = re.sub(r\"[^a-z0-9'\\s]\", '', recode)\n",
    "    return cleaned\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Use NLTK TlktokTokenizer to seperate/tokenize text\n",
    "    '''\n",
    "    # create the NLTK tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(text, return_str=True)\n",
    "\n",
    "def stem(text):\n",
    "    '''\n",
    "    Apply NLTK stemming to text to remove prefix and suffixes\n",
    "    '''\n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    Apply NLTK lemmatizing to text to remove prefix and suffixes\n",
    "    '''\n",
    "    # Create the nltk lemmatize object, then use it\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized\n",
    "\n",
    "def remove_stopwords(text, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    Removes stopwords from text, allows for additional words to exclude, or words to not exclude\n",
    "    '''\n",
    "    # define initial stopwords list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # add additional stopwords\n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    # remove stopwords to exclude from stopword list\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "    # split the string into words\n",
    "    words = text.split()\n",
    "    # filter the words\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    # print number of stopwords removed\n",
    "    # print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    # produce string without stopwords\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "### instructor version\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n",
    "    \n",
    "    return df[['topic', 'title', column, 'stemmed', 'lemmatized', 'clean']]\n",
    "\n",
    "###### my GitHub version\n",
    "def prep_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    # create column with text cleaned\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    # basic clean, tokenize, remove_stopwords, and stem text\n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(stem)\n",
    "    # basic clean, tokenize, remove_stopwords, and lemmatize text\n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "\n",
    "    # add a column with a list of words\n",
    "    words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.lemmatized]\n",
    "\n",
    "    # column name will be words, and the column will contain lists of the words in each doc\n",
    "    df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "    # add column with number of words in readme content\n",
    "    df['doc_length'] = [len(wordlist) for wordlist in df.words]\n",
    "    \n",
    "    # removing unpopular languages \n",
    "    language_list = ['JavaScript', 'Java', 'HTML', 'Python']\n",
    "    df = df[df.language.isin(language_list)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################## Logistic Regression ##########################\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_train_bow, X_train_tfidf):\n",
    "    '''\n",
    "    This function takes in X_train (features using for model) and y_train (target) and performs logistic\n",
    "    regression giving us accuracy of the model and the classification report\n",
    "    '''\n",
    "    # create the object\n",
    "    lm = LogisticRegression()\n",
    "    # fit the object\n",
    "    lm_bow = lm.fit(X_train_bow, y_train)\n",
    "    lm_tfidf = lm.fit(X_train_tfidf, y_train)\n",
    "    # make predictions\n",
    "    X_train['pred_bow'] = lm_bow.predict(X_train_bow)\n",
    "    X_train['pred_tfidf'] = lm_tfidf.predict(X_train_tfidf)\n",
    "    \n",
    "    # X_bow results\n",
    "    print('X_bow Accuracy: {:.0%}\\n'.format(accuracy_score(y_train, X_train.pred_bow)))\n",
    "    print('-----------------------')\n",
    "    print(f'X_bow Confusion Matrix: \\n\\n {pd.crosstab(y_train.language, X_train.pred_bow)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"X_bow Logistic Regression Classification Report:\\n\", classification_report(y_train, X_train.pred_bow))\n",
    "\n",
    "    # TF-IDF results\n",
    "    print('-----------------------')\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_train, X_train.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_train.language, X_train.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Logistic Regression Classification Report:\\n\", classification_report(y_train, X_train.pred_tfidf))\n",
    "    return lm_bow, lm_tfidf\n",
    "\n",
    "\n",
    "######################## Random Forest ##########################\n",
    "\n",
    "def random_forest(X_train, y_train, X_train_bow, X_train_tfidf):\n",
    "    # Random forest object\n",
    "    rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=123)\n",
    "    # Fitting the data to the train data\n",
    "    rf_bow = rf.fit(X_train_bow, y_train)\n",
    "    rf_tfidf = rf.fit(X_train_tfidf, y_train)\n",
    "    # make predictions\n",
    "    X_train['pred_bow'] = rf_bow.predict(X_train_bow)\n",
    "    X_train['pred_tfidf'] = rf_tfidf.predict(X_train_tfidf)\n",
    "\n",
    "    # BOW results\n",
    "    print('X_bow Accuracy: {:.0%}\\n'.format(accuracy_score(y_train.language, X_train.pred_bow)))\n",
    "    print('-----------------------')\n",
    "    print(f'X_bow Confusion Matrix: \\n\\n {pd.crosstab(y_train.language, X_train.pred_bow)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"X_bow Random Forest Classification Report:\\n\", classification_report(y_train.language, X_train.pred_bow))\n",
    "\n",
    "   # TF-IDF results\n",
    "    print('-----------------------')\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_train.language, X_train.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_train.language, X_train.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Random Forest Classification Report:\\n\", classification_report(y_train.language, X_train.pred_tfidf))\n",
    "    return rf_bow, rf_tfidf\n",
    "\n",
    "\n",
    "######################## Complement Naive Bayes ##########################\n",
    "\n",
    "def complement_naive_bayes(X_train, y_train, X_train_tfidf):\n",
    "    \n",
    "    # create object, fit, and predict\n",
    "    cnb = ComplementNB(alpha=1.0)\n",
    "    cnb_tfidf = cnb.fit(X_train_tfidf, y_train)\n",
    "    X_train['pred_tfidf'] = cnb_tfidf.predict(X_train_tfidf)\n",
    "\n",
    "    # TF-IDF results\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_train, X_train.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_train.language, X_train.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Complement Niave Bayes Classification Report:\\n\", classification_report(y_train, X_train.pred_tfidf))\n",
    "    return cnb_tfidf\n",
    " \n",
    "######################## Validate Logistic Regression ##########################\n",
    "\n",
    "def validate_logistic_regression(X_validate, y_validate, X_val_bow, X_val_tfidf, lm_bow, lm_tfidf):\n",
    "    '''\n",
    "    This function takes in X_train (features using for model) and y_train (target) and performs logistic\n",
    "    regression giving us accuracy of the model and the classification report\n",
    "    '''\n",
    "    # create predictions\n",
    "    X_validate['pred_bow'] = lm_bow.predict(X_val_bow)\n",
    "    X_validate['pred_tfidf'] = lm_tfidf.predict(X_val_tfidf)\n",
    "\n",
    "    # X_bow results\n",
    "    print('X_bow Accuracy: {:.0%}\\n'.format(accuracy_score(y_validate, X_validate.pred_bow)))\n",
    "    print('-----------------------')\n",
    "    print(f'X_bow Confusion Matrix: \\n\\n {pd.crosstab(y_validate.language, X_validate.pred_bow)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"X_bow Logistic Regression Classification Report:\\n\", classification_report(y_validate, X_validate.pred_bow))\n",
    "\n",
    "    # TF-IDF results\n",
    "    print('-----------------------')\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_validate, X_validate.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_validate.language, X_validate.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Logistic Regression Classification Report:\\n\", classification_report(y_validate, X_validate.pred_tfidf))\n",
    "\n",
    "######################## Validate Random Forest ##########################\n",
    "\n",
    "def validate_random_forest(X_validate, y_validate, X_val_bow, X_val_tfidf, rf_bow, rf_tfidf):\n",
    "    # create predictions\n",
    "    X_validate['pred_bow'] = rf_bow.predict(X_val_bow)\n",
    "    X_validate['pred_tfidf'] = rf_tfidf.predict(X_val_tfidf)\n",
    "    \n",
    "    # X_bow results\n",
    "    print('X_bow Accuracy: {:.0%}\\n'.format(accuracy_score(y_validate.language, X_validate.pred_bow)))\n",
    "    print('-----------------------')\n",
    "    print(f'X_bow Confusion Matrix: \\n\\n {pd.crosstab(y_validate.language, X_validate.pred_bow)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"X_bow Random Forest Classification Report:\\n\", classification_report(y_validate.language, X_validate.pred_bow))\n",
    "\n",
    "   # TF-IDF results\n",
    "    print('-----------------------')\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_validate.language, X_validate.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_validate.language, X_validate.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Random Forest Classification Report:\\n\", classification_report(y_validate.language, X_validate.pred_tfidf))\n",
    "\n",
    "######################## Validate Complement Naive Bayes ##########################\n",
    "\n",
    "def validate_complement_naive_bayes(X_validate, y_validate, X_val_tfidf, cnb_tfidf):\n",
    "    # makes predictions\n",
    "    X_validate['pred_tfidf'] = cnb_tfidf.predict(X_val_tfidf)\n",
    "\n",
    "    # TF-IDF results\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_validate, X_validate.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_validate.language, X_validate.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Complement Niave Bayes Classification Report:\\n\", classification_report(y_validate, X_validate.pred_tfidf))\n",
    "\n",
    "######################## Test  ##########################\n",
    "\n",
    "def test_random_forest(X_test, y_test, X_test_tfidf, rf_tfidf):\n",
    "    # Creaye predictions\n",
    "    X_test['pred_tfidf'] = rf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "   # Confusion matrix\n",
    "    print('TF-IDF Accuracy: {:.0%}\\n'.format(accuracy_score(y_test.language, X_test.pred_tfidf)))\n",
    "    print('-----------------------')\n",
    "    print(f'TF-IDF Confusion Matrix: \\n\\n {pd.crosstab(y_test.language, X_test.pred_tfidf)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"TF-IDF Random Forest Classification Report:\\n\", classification_report(y_test.language, X_test.pred_tfidf))\n",
    "\n",
    "def test_logistic_regression(X_test, y_test, X_test_bow, lm_bow):\n",
    "    # Create prediction\n",
    "    X_test['pred_bow'] = lm_bow.predict(X_test_bow)\n",
    "\n",
    "    # X_bow results\n",
    "    print('X_bow Accuracy: {:.0%}\\n'.format(accuracy_score(y_test, X_test.pred_bow)))\n",
    "    print('-----------------------')\n",
    "    print(f'X_bow Confusion Matrix: \\n\\n {pd.crosstab(y_test.language, X_test.pred_bow)}\\n' )\n",
    "    print('-----------------------')\n",
    "    print(\"X_bow Logistic Regression Classification Report:\\n\", classification_report(y_test, X_test.pred_bow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
