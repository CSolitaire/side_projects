import unicodedata
import re
import json

import nltk
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.corpus import stopwords

import pandas as pd

import acquire



###################################### Clean Data Function ###############################

def basic_clean(string):
    '''
    Initial basic cleaning/normalization of text string
    '''
    # change to all lowercase
    low_case = string.lower()
    # remove special characters, encode to ascii and recode to utf-8
    recode = unicodedata.normalize('NFKD', low_case).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    # Replace anything that is not a letter, number, whitespace or a single quote
    cleaned = re.sub(r"[^a-z0-9'\s]", '', recode)
    # Remove numbers from text
    text_cleaned = re.sub(r'\d+', '', cleaned)
    return text_cleaned

def tokenize(text_cleaned):
    '''
    This function takes in a string and
    returns a tokenized string.
    '''
    # Create tokenizer.
    tokenizer = nltk.tokenize.ToktokTokenizer()
    
    # Use tokenizer
    text_tokenized = tokenizer.tokenize(text_cleaned, return_str=True)
    
    return text_tokenized

def lemmatize(text_tokenized):
    '''
    This function takes in string for and
    returns a string with words lemmatized.
    '''
    # Create the lemmatizer.
    wnl = nltk.stem.WordNetLemmatizer()
    
    # Use the lemmatizer on each word in the list of words we created by using split.
    lemmas = [wnl.lemmatize(word) for word in text_tokenized.split()]
    
    # Join our list of words into a string again and assign to a variable.
    text_lemmatized = ' '.join(lemmas)
    
    return text_lemmatized


def remove_stopwords(text_lemmatized, extra_words=['accuracy',
    'accurate',
    'acknowledgement',
    'acquired',
    'adhere',
    'adheres',
    'along',
    'also',
    'accessing',
    'provided',
    'directory',
    'datasets',
    'including',
    'pip',
    'problem',
    'pull',
    'python',
    'according',
    'account',
    'across',
    'addition',
    'additional',
    'aim',
    'anaconda',
    'analyse',
    'analysed',
    'analysis',
    'andor',
    'answer',
    'public',
    'proxy',
    'properly',
    'prompt',
    'program',
    'produce',
    'produced',
    'reference',
    'need',
    'optimization',
    'viewer',
    'open',
    'norequestrendermode',
    'preform',
    'performed',
    'point',
    'pointcloud',
    'question',
    'install',
    'data',
    'import',
    'using',
    'please',
    'different',
    'installation',
    'preprocessing',
    'library',
    'type',
    'implemented',
    'minimum',
    'github',
    'run',
    'file',
    'license',
    'result',
    'record',
    'possible',
    'number',
    'could',
    'see',
    'use',
    'source',
    'define',
    'parameter',
    'geology',
    'disable',
    'limit',
    'inspector',
    'quality',
    'default',
    'display',
    'activate',
    'alternative',
    'anacona',
    'associated',
    'assumption',
    'attempt',
    'author',
    'automated',
    'automatic',
    'available',
    'purpose',
    'published',
    'publically',
    'prevented',
    'navigation',
    'nolimit',
    'package',
    'al',
    'application',
    'assetids',
    'based',
    'better',
    'bias',
    'biased',
    'birthdeath',
    'raw',
    'rather',
    'present',
    'prerequisitewindowv',
    'particular',
    'partially',
    'oxford',
    'requires',
    'happy',
    'request',
    'report',
    'high',
    'provide',
    'project',
    'processing',
    'potential',
    'calculated',
    'cannot',
    'capture',
    'carried',
    'cause',
    'cc',
    'cd',
    'conda',
    'variety',
    'notebook',
    'work',
    'image',
    'sample',
    'check',
    'running',
    'console',
    'example',
    'easily',
    'map',
    'sampling',
    'true',
    'method',
    'version',
    'script',
    'many',
    'set',
    'url',
    'ngm',
    'npm',
    'swissgeol',
    'swisstopo',
    'list',
    'behavior',
    'git',
    'licencemd',
    'restrict',
    'swiss',
    'mind',
    'initialscreenspaceerror',
    'gdal',
    'following',
    'welcomed',
    'terminal',
    'workflow',
    'cloud',
    'review',
    'jupyter',
    'laboratory',
    'technique',
    'navigate',
    'test',
    'time',
    'used',
    'pde',
    'reforester',
    'database',
    'past',
    'research',
    'section',
    'visual',
    'developingmd',
    'ownterrainfalse',
    'widget',
    'json',
    'cpu',
    'new',
    'welcome',
    'id',
    'integrated',
    'opening',
    'learning',
    'code',
    'marker',
    'corrected',
    'trying',
    'citing',
    'though',
    'refer',
    'history',
    'gnu',
    'specie',
    'dataimporterr',
    'dissertation',
    'wa',
    'httpsbetaswissgeolch',
    'layout',
    'software',
    'swissrectangle',
    'localhost',
    'except',
    'start',
    'httplocalhost',
    'keyboard',
    'disables',
    'us',
    'machine',
    'server',
    'documentation',
    'testing',
    'integration',
    'cool',
    'create',
    'stay',
    'due',
    'study',
    'estimate',
    'pattern',
    'general',
    'error',
    'enforce',
    'made',
    'started',
    'local',
    'clone',
    'modify',
    'sphere',
    'coma',
    'getting',
    'nolimitfalse',
    'rectangle',
    'maximumscreenspaceerror',
    'io',
    'fix',
    'userhylite',
    'dimensionality',
    'ioloadtestdatahypercloudplycloudquickplotcloudheadergetcamera',
    'cite',
    'hylitergb',
    'specific',
    'thiele',
    'lib',
    'pdes',
    'included',
    'occurrence',
    'recent',
    'must',
    'significant',
    'texture',
    'httpsopengameartorgcontenttemplateorangetexturepack',
    'swissrectanglefalse',
    'note',
    'launch',
    'conventionally',
    'mark',
    'technology',
    'fast',
    'opencv',
    'marking',
    'take',
    'efficiently',
    'sure',
    'make',
    'solved',
    'unsupervised',
    'know',
    'long',
    'matplotlib',
    'numpy',
    'created',
    'imagine',
    'longheld',
    'word',
    'phytools',
    'placed',
    'eg',
    'indicate',
    'complete',
    'received',
    'term',
    'results',
    'format',
    'desired',
    'templatelogistic',
    'template',
    'r',
    'keyboardlayouteditor',
    'httpsgithubcomswissgeolngmgit',
    'dev',
    'dtilesets',
    'touchdirectly',
    'step',
    'issue',
    'correction',
    'highresolution',
    'find',
    'derivative',
    'geological',
    'want',
    'scan',
    'supervised',
    'ioloadtestdatalibrarycsvlibquickplot',
    'information',
    'level',
    'exponentially',
    'optional',
    'implied',
    'google',
    'thing',
    'idea',
    'said',
    'timeheterogeneous',
    'entierty',
    'inferred',
    'tidyverse',
    'detailed',
    'roger',
    'still',
    'estimation',
    'appearance',
    'next',
    'ground',
    'opensource',
    'typing',
    'corescannerhypercloud',
    'one',
    'design',
    'et',
    'httpshylitereadthedocsioenlatestindexhtml',
    'openpit',
    'installed',
    'scene',
    'templatenotebooks',
    'difficult',
    'setuppy',
    'significantly',
    'benson',
    'even',
    'referred',
    'compared',
    'lineagecombining',
    'inverse',
    'daniel',
    'regime',
    'vast',
    'function',
    'functionsuniformsamplingr',
    'factor',
    'fine',
    'equilibrium',
    'length',
    'myrs',
    'myr',
    'empirical',
    'doe',
    'includes',
    'launching',
    'try',
    'contributing',
    'generate',
    'multiscale',
    'simple',
    'feature',
    'perform',
    'lorenz',
    'respectively',
    'merchantability',
    'order',
    'end',
    'least',
    'marineeumetazoacsv',
    'window',
    'ubiquity',
    'second',
    'completeness',
    'theoretically',
    'originatorclose',
    'hope',
    'common',
    'operation',
    'similarly',
    'value',
    'inaccurate',
    'thereby',
    'tuned',
    'currently',
    'unzip',
    'ioloadtestdataimagehdrimagequickplothylitergb',
    'submit',
    'setuptools',
    'repository',
    'seamless',
    'commented',
    'prerequisitev',
    'exponentiallydiversifying',
    'httpswwwgnuorglicenses',
    'consistently',
    'ha',
    'viewed',
    'suggests',
    'slight',
    'introduces',
    'closer',
    'maximum',
    'unfortunately',
    'billie',
    'readme',
    'caper',
    'restriction',
    'free',
    'extend',
    'reduction',
    'downloading',
    'feel',
    'get',
    'hyperclouds',
    'download',
    'key',
    'behaviour',
    'thompson',
    'style',
    'count',
    'sufficiently',
    'earth',
    'specifically',
    'functionssavesetsr',
    'copytxt',
    'redistribute',
    'foundation',
    'way',
    'reinterpretation',
    'rigorous',
    'ouput',
    'enviroment',
    'corescannerhypercloud',
    'userhylite'
    'required'], exclude_words=[]):
    '''
    This function takes in a string, optional extra_words and exclude_words parameters
    with default empty lists and returns a string.
    '''
    # define initial stopwords list
    stopword_list = stopwords.words('english')
    # add additional stopwords
    for word in extra_words:
        stopword_list.append(word)
    # remove stopwords to exclude from stopword list
    for word in exclude_words:
        stopword_list.remove(word)
    # split the string into words
    words = text_lemmatized.split()
    # filter the words
    filtered_words = [w for w in words if w not in stopword_list]
    # print number of stopwords removed
    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))
    # produce string without stopwords
    article_without_stopwords = ' '.join(filtered_words)
    return article_without_stopwords


###### my GitHub version
def prep_data(df, column):
    '''
    This function take in a df and the string name for a text column with 
    option to pass lists for extra_words and exclude_words and
    returns a df with the text article title, original text, stemmed text,
    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.
    '''
    # Formatts language makes it easier to read
    df['text_cleaned'] = df.content.apply(basic_clean)
    df['text_tokenized'] = df.text_cleaned.apply(tokenize)
    df['text_lemmatized'] = df.text_tokenized.apply(lemmatize)
    df['text_filtered'] = df.text_lemmatized.apply(remove_stopwords)
    # Add column with list of words
    words = [re.sub(r'([^a-z0-9\s]|\s.\s)', '', doc).split() for doc in df.text_filtered]
    df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)
    # Adds colum with lenght of word list
    df['doc_length'] = [len(wordlist) for wordlist in df.words]
      # removing unpopular languages 
    language_list = ['JavaScript', 'R', 'Jupyter Notebook']
    df = df[df.language.isin(language_list)]
    return df

  